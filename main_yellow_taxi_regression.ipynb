{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9517912,"sourceType":"datasetVersion","datasetId":5794654}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/niltonpegass/yellow-taxi-regression-fare-predict?scriptVersionId=241806574\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"import warnings\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport datetime as dt\n\nfrom scipy import stats\n\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.model_selection import GridSearchCV, train_test_split, StratifiedKFold, KFold\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\nfrom xgboost import XGBClassifier, XGBRegressor\nimport xgboost as xgb\nimport lightgbm as lgb\n\nfrom sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error\nfrom sklearn.metrics import roc_auc_score, roc_curve\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score,f1_score, confusion_matrix, ConfusionMatrixDisplay, RocCurveDisplay\nfrom xgboost import plot_importance\nimport sklearn.metrics as metrics\n\npd.set_option('display.max_columns', None)\nwarnings.filterwarnings('ignore')\n\ndf = pd.read_csv('/kaggle/input/new-york-city-taxi-trips-2017/2017_Yellow_Taxi_Trip_Data.csv')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def display_dataset_overview(df: pd.DataFrame) -> None:\n    \"\"\"\n    Display general information about the dataset,\n    including shape, variable descriptions, sample data, and summary statistics.\n    \"\"\"\n    print(\n        f\">> Dataset shape: {df.shape[0]} rows and {df.shape[1]} columns \"\n        f\"({df.shape[0] * df.shape[1]} data points)\\n\"\n    )\n\n    # Description of key dataset variables\n    variable_descriptions = {\n        'ab': \"ab\",\n        'ab': \"ab\",\n        'ab': \"ab\",\n        'ab': \"ab\",\n        'ab': \"ab\",\n        'ab': \"ab\",\n        'ab': \"ab\",\n        'ab': \"ab\",\n        'ab': \"ab\",\n        'ab': \"ab\"\n    }\n\n    variable_df = (\n        pd.DataFrame.from_dict(variable_descriptions, orient='index', columns=['Description'])\n        .reset_index()\n        .rename(columns={'index': 'Variable'})\n    )\n    display(variable_df)\n\n    print(\">> General dataset information:\")\n    display(df.info())\n\n    print(\">> First 5 rows:\")\n    display(df.head(5))\n\n    print(\">> Descriptive statistics:\")\n    display(df.describe())\n\n\n# Run the overview on the processed dataset\ndisplay_dataset_overview(df)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df['tpep_pickup_datetime'] = pd.to_datetime(df['tpep_pickup_datetime'])\ndf['tpep_dropoff_datetime'] = pd.to_datetime(df['tpep_dropoff_datetime'])\n\ndf['month_num'] = df['tpep_pickup_datetime'].dt.strftime('%m')\ndf['month_name'] = df['tpep_pickup_datetime'].dt.month_name().str[:3].str.lower()\n\ndf['day_num'] = df['tpep_pickup_datetime'].dt.strftime('%d')\ndf['day_name'] = df['tpep_pickup_datetime'].dt.day_name().str[:3].str.lower()\n\ndisplay(df.head(10))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"months = ['jan', 'feb', 'mar', 'apr', 'may', 'jun', 'jul', 'aug', 'sep', 'oct', 'nov', 'dec']\ndays = ['sun', 'mon', 'tue', 'wed', 'thu', 'fri', 'sat']\n\ndisplay(df['passenger_count'].value_counts())\n\nprint()\n\nrides_day = df['day_name'].value_counts().reindex(index = days)\ndisplay(rides_day)\n\nprint()\n\ndf_group_sum_revenue_day = (df.groupby('day_name')[['total_amount']]\n            .sum()\n            .round(2)\n            .reindex(index = days))\ndisplay(df_group_sum_revenue_day)\n\nprint()\n\nrides_month = df['month_name'].value_counts().reindex(index = months)\ndisplay(rides_month)\n\ndf_group_mean_tips = (df.groupby(['passenger_count'])[['tip_amount', 'total_amount']]\n            .mean()\n            .sort_values(by = 'passenger_count', ascending = False)\n            .round(2))\ndisplay(df_group_mean_tips)\n\nprint()\n\ndf_group_sum_revenue_month = (df.groupby('month_name')[['total_amount']]\n            .sum()\n            .round(2)\n            .reindex(index = months))\ndisplay(df_group_sum_revenue_month)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize = (7, 3))\n\nsns.boxplot(x = df['month_name'],\n            y = df['trip_distance'],\n            order = months,\n            showfliers = False);\n\nplt.xlabel('month', fontsize = 9, weight = 'bold')\nplt.ylabel('trip distance', fontsize = 9, weight = 'bold')\nplt.title('trip distance x month', fontsize = 12)\nplt.xticks(rotation = 40)\n\nplt.show()\n\n# mask_trip_distance = (df['trip_distance'] <= 4)\n# df_mask_trip_distance = df[mask_trip_distance]\n\nplt.figure(figsize = (12, 5))\nsns.histplot(data = df,\n             x = 'trip_distance',\n             bins = 60,\n             kde = True)\n\nplt.title('histogram - trip distance', fontsize = 12)\nplt.grid(True)\n\nplt.figure(figsize = (7, 7))\nsns.boxplot(x = df['month_name'],\n            y = df['total_amount'],\n            order = months,\n            showfliers = True);\n\nplt.xlabel('month', fontsize = 9, weight = 'bold')\nplt.ylabel('total amount', fontsize = 9, weight = 'bold')\nplt.title('total amount x month', fontsize = 12)\nplt.xticks(rotation = 40)\n\nplt.show()\n\nmask_total_amount = ((df['total_amount'] <= 200) & (df['total_amount'] >= 0))\ndf_mask_total_amount = df[mask_total_amount]\n\nplt.figure(figsize = (12, 5))\nsns.histplot(data = df_mask_total_amount,\n             x = 'total_amount',\n             bins = 60,\n             kde = True)\n\nplt.title('histogram - total amount', fontsize = 12)\nplt.grid(True)\n\nplt.figure(figsize = (7, 7))\nsns.boxplot(x = df['month_name'],\n            y = df['tip_amount'],\n            order = months,\n            showfliers = True)\n\nplt.xlabel('month', fontsize = 9, weight = 'bold')\nplt.ylabel('tip amount', fontsize = 9, weight = 'bold')\nplt.title('tip amount x month', fontsize = 12)\nplt.xticks(rotation = 40)\n\nplt.show()\n\nplt.figure(figsize = (12, 5))\nsns.histplot(data = df,\n             x = 'tip_amount',\n             bins = 100,\n             kde = False)\n\nplt.title('histogram - tip amount', fontsize = 12)\nplt.grid(True)\n\nplt.figure(figsize = (12, 5))\ndata = df_group_sum_revenue_month.head(12)\npal = sns.color_palette(\"Blues_d\", len(data))\nrank = data['total_amount'].argsort()\nax = sns.barplot(x = data.index,\n                 y = data['total_amount'],\n                 palette = np.array(pal[::-1])[rank])\nax.axhline(df_group_sum_revenue_month['total_amount'].mean(), ls = '-', color = 'red', label = 'global mean')\nax.legend()\nplt.title('revenue per month', fontsize = 12)\nplt.xlabel('month', fontsize = 9)\nplt.ylabel('total amount', fontsize = 9)\nplt.grid(True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dropoff_mean = df.groupby('DOLocationID')[['trip_distance']].mean()\ndropoff_mean = dropoff_mean.sort_values(by = 'trip_distance', ascending = True)\ndisplay(dropoff_mean)\n\nplt.figure(figsize = (12, 7))\nax = sns.barplot(x = dropoff_mean.index,\n            y = dropoff_mean['trip_distance'],\n            order = dropoff_mean.index)\nax.set_xticklabels([])\nax.set_xticks([])\nplt.title('mean trip distance by drop-off location', fontsize = 12)\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"flag_order = ['Y', 'N']\n\ndf_sorted_total_amount = df\ndf_sorted_total_amount['total_amount'] = df_sorted_total_amount['total_amount'].sort_values(ascending = True)\ndf_sorted_total_amount\n\nplt.figure(figsize = (12, 7))\nax = sns.boxplot(x = df_sorted_total_amount['total_amount'],\n                 y = df_sorted_total_amount['store_and_fwd_flag'],\n                 order = flag_order)\n\n#ax.set_xticklabels(df_sorted_total_amount['total_amount'], fontsize = 16)\n#ax.set_yticklabels(df_sorted_total_amount['store_and_fwd_flag'], fontsize = 16)\nplt.xlabel('total amount', fontsize = 9)\nplt.ylabel('flag store', fontsize = 9)\n\nplt.grid()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"***Pro tip:*** Put yourself in your client's perspective, what would they want to know? \n\nMy client would likely want to know: if the type of outliers come from the way that this data was set up on the datasource, if it was sent with the flag Y or N, which can be a source of data inconsistency.","metadata":{}},{"cell_type":"code","source":"df['trip_duration'] = ((df['tpep_dropoff_datetime'] - df['tpep_pickup_datetime']).dt.total_seconds() / 60).round(2) #trip_duration [min]\ndf[['tpep_pickup_datetime', 'tpep_dropoff_datetime', 'trip_duration', 'total_amount', 'month_name', 'day_name']]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def calculate_iqr_limits(df, column):\n    ### Calcula os limites superior e inferior usando IQR para uma coluna específica\n    \n    q1 = df[column].quantile(0.25)\n    q3 = df[column].quantile(0.75)\n    iqr = q3 - q1\n    upper_limit = q3 + 1.5 * iqr\n    lower_limit = q1 - 1.5 * iqr\n    return q1, q3, iqr, upper_limit, lower_limit\n\ndef view_outliers(df):\n    \n    numeric_columns = df.select_dtypes(include=['number']).columns\n    total_dataset = df.shape[0]\n    \n    for column in numeric_columns:\n        q1, q3, iqr, upper_limit, lower_limit = calculate_iqr_limits(df, column)\n        \n        df_outliers = df[(df[column] >= upper_limit) | (df[column] <= lower_limit)]\n        \n        total_outliers = df_outliers.shape[0]\n        percentage_outliers = total_outliers / total_dataset\n        \n        if 0 < percentage_outliers < 1:\n            print(f'[ {column} ] >> {total_outliers} outliers ::: {percentage_outliers * 100:.2f}%')\n            \nview_outliers(df)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"column_list = ['trip_distance', 'fare_amount', 'trip_duration']\n\nfig, axes = plt.subplots(1, 3, figsize = (15, 5))\nfig.suptitle('Outlier Visualization', fontsize = 14)\n\nfor i, column in enumerate(column_list):\n    sns.boxplot(ax = axes[i], x = df[column])\n    axes[i].set_title(column)\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def fix_data(df, column, factor_limit):\n    \n    q1, q3, iqr, upper_limit, lower_limit = calculate_iqr_limits(df, column)\n    superior_limit = q3 + factor_limit * iqr # Define um limite superior\n\n    print(f'>> {column} column adjustment')\n    print(f'>> Q3: {q3:.2f}')\n    print(f'>> IQR: {iqr:.2f}')\n    print(f'>> Superior Limit (custom): {superior_limit:.2f}\\n')\n    \n    df.loc[df[column] < 0, column] = 0 # Ajusta valores negativos para 0\n    df.loc[df[column] > superior_limit, column] = superior_limit # Ajusta valores que ultrapassam superior_limit\n    \n    display(df[column].describe())\n    print()\n\nfor column in column_list:\n    fix_data(df = df, column = column, factor_limit = 6)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## A/B Test","metadata":{}},{"cell_type":"markdown","source":"***down:*** You are interested in the relationship between payment type and the fare amount the customer pays. One approach is to look at the average fare amount for each payment type. ","metadata":{}},{"cell_type":"code","source":"df_credit = df[df['payment_type'] == 1]\ndf_cash = df[df['payment_type'] == 2]\n\nmean_credit = df_credit['fare_amount'].mean()\nmean_cash = df_cash['fare_amount'].mean()\n\nstd_credit = df_credit['fare_amount'].std()\nstd_cash = df_cash['fare_amount'].std()\n\nprint(f'mean fare amount [credit]: {mean_credit:.2f} // mean fare amount [cash]: {mean_cash:.2f}')\nprint(f'std fare amount [credit]: {std_credit:.2f} // std fare amount [cash]: {std_cash:.2f}')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Based on the averages shown, it appears that customers who pay in credit card tend to pay a larger fare amount than customers who pay in cash. However, this difference might arise from random sampling, rather than being a true difference in fare amount. To assess whether the difference is statistically significant, we will conduct a hypothesis test.","metadata":{}},{"cell_type":"markdown","source":"#### Task 3. Hypothesis testing\n\nBefore you conduct the hypothesis test, consider the following questions where applicable to complete:\n\nRecall the difference between the null hypothesis and the alternative hypotheses. Consider the hypotheses for this project as listed below.\n\n\n$H_0$: There is no difference in the average fare amount between customers who use credit cards and customers who use cash.\n\n$H_A$: There is a difference in the average fare amount between customers who use credit cards and customers who use cash.","metadata":{}},{"cell_type":"code","source":"statistic, pvalue = stats.ttest_ind(a = df_credit['fare_amount'],\n                                    b = df_cash['fare_amount'],\n                                    equal_var = False)\nprint(f'statistic: {statistic:.2f} // pvalue: {pvalue * 100:.2f}%')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The pvalue is significantly smaller than the significance level, then we reject the null hypothesis. Our analysis shows us that there is a significant difference for each payment type. Even if we had chosen 1% for the significance level, the analysis gives support to the rejection of the null hypothesis.","metadata":{}},{"cell_type":"markdown","source":"### Feature Engineering","metadata":{}},{"cell_type":"markdown","source":"#### Create `mean_distance` and `mean_duration` column\n\nWhen deployed, the model will not know the duration of a trip until after the trip occurs, so we cannot train a model that uses this feature. However, we can use the statistics of trips we *do* know to generalize about ones we do not know.\n\nIn this step, we will create a column called `mean_distance` that captures the mean distance for each group of trips that share pickup and dropoff points.\n\nFor example, if your data were:\n\n| Trip   | Start  |  End   |Distance|\n|:------:|:------:|:------:|:------:|\n|  1     |   A    |   B    |    1   |\n|  2     |   C    |   D    |    2   |\n|  3     |   A    |   B    |   1.5  |\n|  4     |   D    |   C    |    3   |\n\nThe results should be:\n```\nA -> B: 1.25 miles\nC -> D: 2 miles\nD -> C: 3 miles\n```\n\nThen, a new column `mean_distance` will be added where the value at each row is the average for all trips with those pickup and dropoff locations:\n\n| Trip   | Start  |  End   |Distance|mean_distance|\n|:------:|:------:|:------:|:------:|:-----------:|\n|  1     |   A    |   B    |    1   |   1.25      |\n|  2     |   C    |   D    |    2   |    2        |\n|  3     |   A    |   B    |   1.5  |   1.25      |\n|  4     |   D    |   C    |    3   |    3        |\n\n\nBegin by creating a helper column called `pickup_dropoff`, which contains the unique combination of pickup and dropoff location IDs for each row.","metadata":{}},{"cell_type":"code","source":"df['pickup_dropoff'] = df['PULocationID'].astype('str') + ' ' + df['DOLocationID'].astype('str')\ndf[['pickup_dropoff']].head(3)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"column_list = ['trip_distance', 'trip_duration']\n\nfor i, column in enumerate(column_list):\n    grouped = df.groupby('pickup_dropoff').mean(numeric_only = True)[[column]].round(4)\n        \n    grouped_dict = grouped.to_dict()\n    grouped_dict = grouped_dict[column]\n    \n    df['mean_' + column] = df['pickup_dropoff']\n    df['mean_' + column] = df['mean_' + column].map(grouped_dict)\n\nprint(df[['mean_' + col for col in column_list]])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def rush_transformation(row):\n    \n    condition_wknd = row['day_name'] not in ['sat', 'sun']\n    cond_06_10 =  6 <= row['tpep_pickup_datetime'].hour <= 10\n    cond_16_20 = 16 <= row['tpep_pickup_datetime'].hour <= 20\n    conditions = condition_wknd and (cond_06_10 or cond_16_20)\n    \n    return int(conditions)\n\ndf['rush_hour'] = df.apply(rush_transformation, axis = 1)\ndf[['day_name', 'tpep_pickup_datetime', 'rush_hour']]\n\ndef day_period(row):\n    pickup_hour = row['tpep_pickup_datetime'].hour\n    if 6 <= pickup_hour < 10:\n        return 'am_rush'\n    elif 10 <= pickup_hour < 16:\n        return 'daytime'\n    elif 16 <= pickup_hour < 20:\n        return 'pm_rush'\n    else:\n        return 'nighttime'\n        \ndf['day_period'] = df.apply(day_period, axis = 1)\ndf.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_implemented = df[[\n    'VendorID', 'passenger_count', 'fare_amount', 'mean_trip_distance',\n    'mean_trip_duration','day_name', 'day_period', 'rush_hour']].copy()\n\ndf_implemented['VendorID'] = df_implemented['VendorID'].astype(str)\ndf_implemented = pd.get_dummies(df_implemented, drop_first = False)\n\nX = df_implemented.drop(columns = ['fare_amount'])\ny = df_implemented[['fare_amount']]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\nlr = LinearRegression()\nlr.fit(X_train_scaled, y_train)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sns.pairplot(df_implemented[['fare_amount', 'mean_trip_duration', 'mean_trip_distance']],\n            plot_kws={'alpha': 0.3, 'size': 3});\n\nplt.figure(figsize = (12, 5))\nsns.heatmap(df_implemented[[\n    'fare_amount', 'mean_trip_duration', 'mean_trip_distance', 'passenger_count',\n    'VendorID_1', 'VendorID_2', 'rush_hour', 'day_name_sun', 'day_name_mon'\n        ]].corr(method = 'pearson'),\n            annot = True,\n            cmap = 'Blues')\nplt.title('correlation heatmap', fontsize = 12)\n\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"`mean_duration` and `mean_distance` are both highly correlated with the target variable of `fare_amount` They're also both correlated with each other, with a Pearson correlation of 0.89.\n\nRecall that highly correlated predictor variables can be bad for linear regression models when you want to be able to draw statistical inferences about the data from the model. However, correlated predictor variables can still be used to create an accurate predictor if the prediction itself is more important than using the model as a tool to learn about your data.\n\nThis model will predict `fare_amount`, which will be used as a predictor variable in machine learning models. Therefore, we will try modeling with both variables even though they are correlated.","metadata":{}},{"cell_type":"markdown","source":"### Train data","metadata":{}},{"cell_type":"code","source":"y_pred_train = lr.predict(X_train_scaled)\n\nprint('R^2:', r2_score(y_train, y_pred_train))\nprint('MAE:', mean_absolute_error(y_train, y_pred_train))\nprint('MSE:', mean_squared_error(y_train, y_pred_train))\nprint('RMSE:', np.sqrt(mean_squared_error(y_train, y_pred_train)))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Test data","metadata":{}},{"cell_type":"code","source":"X_test_scaled = scaler.transform(X_test)\ny_pred_test = lr.predict(X_test_scaled)\n\nprint('R^2:', r2_score(y_test, y_pred_test))\nprint('MAE:', mean_absolute_error(y_test,y_pred_test))\nprint('MSE:', mean_squared_error(y_test, y_pred_test))\nprint('RMSE:',np.sqrt(mean_squared_error(y_test, y_pred_test)))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"results = pd.DataFrame(data = {'actual': y_test['fare_amount'],\n                               'predicted': y_pred_test.ravel()})\nresults['residual'] = results['actual'] - results['predicted']\nresults.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The model performance is high on both training and test sets, suggesting that there is little bias in the model and that the model is not overfit. In fact, the test scores were even better than the training scores.\n\nFor the test data, an R<sup>2</sup> of 87.25% of the variance in the `fare_amount` variable is described by the model.\n\nThe mean absolute error is informative here because, for the purposes of the model, an error of two is not more than twice as bad as an error of one.","metadata":{}},{"cell_type":"markdown","source":"### Visualize model results","metadata":{}},{"cell_type":"code","source":"fig, axes = plt.subplots(1, 3, figsize = (18, 6))\nsns.set(style = 'whitegrid')\n\n# Gráfico 1: actual vs. predicted\nsns.scatterplot(\n    x = 'actual',\n    y = 'predicted',\n    data = results,\n    s = 20,\n    alpha = 0.5,\n    ax = axes[0])\n\naxes[0].plot([0, 60], [0, 60], c = 'red', linewidth = 2)\naxes[0].set_title('Actual vs. Predicted')\n\n# Gráfico 2: Distribuição dos resíduos\nsns.histplot(results['residual'],\n             bins = np.arange(-15, 15.5, 0.5),\n             ax = axes[1])\naxes[1].set_title('Distribution of the Residuals')\n\n# Gráfico 3: Resíduos sobre valores previstos\nsns.scatterplot(\n    x = 'predicted',\n    y = 'residual',\n    data = results,\n    ax = axes[2]\n)\naxes[2].axhline(0, c = 'red')\naxes[2].set_title('Residuals over Predicted Values')\naxes[2].set_xlabel('Predicted')\naxes[2].set_ylabel('Residual')\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The distribution of the residuals is approximately normal and has a mean of -0.015. The residuals represent the variance in the outcome variable that is not explained by the model. A normal distribution around zero is good, as it demonstrates that the model's errors are evenly distributed and unbiased.\n\nThe model's residuals are evenly distributed above and below zero, with the exception of the sloping lines from the upper-left corner to the lower-right corner, which you know are the imputed maximum of \\\\$62.50 and the flat rate of \\\\$52 for JFK airport trips.","metadata":{}},{"cell_type":"code","source":"coefficients = pd.DataFrame(lr.coef_, columns = X.columns)\ncoefficients","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The coefficients reveal that `mean_distance` was the feature with the greatest weight in the model's final prediction. Be careful here! A common misinterpretation is that for every mile traveled, the fare amount increases by a mean of \\\\$7.57. This is incorrect. Remember, the data used to train the model was standardized with `StandardScaler()`. As such, the units are no longer miles. In other words, we cannot say \"for every mile traveled...\", as stated above. The correct interpretation of this coefficient is: controlling for other variables, *for every +1 change in standard deviation*, the fare amount increases by a mean of \\\\$7.57. Note also that because some highly correlated features were not removed, the confidence interval of this assessment is wider.\n\nTranslate this back to miles instead of standard deviation (i.e., unscale the data):\n\n1. Calculate the standard deviation of `mean_distance` in the `X_train` data.\n2. Divide the coefficient by the result to yield a more intuitive interpretation.","metadata":{}},{"cell_type":"code","source":"# ## 1. Calculate SD of `mean_distance` in X_train data\n# ## 2. Divide the model coefficient by the standard deviation\nvalue1 = coefficients['mean_trip_distance']\nvalue2 = (coefficients['mean_trip_distance'] / X_train['mean_trip_distance'].std())\n\nprint(f\">> the fare increased by a mean of ${value1.item():.2f} for every {X_train['mean_trip_distance'].std():.2f} miles traveled\")\nprint(f\">> and by a mean of ${value2.item():.2f} for every 1 mile traveled\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Predict on full dataset\nSave final predictions with `mean_duration` and `mean_distance` columns","metadata":{}},{"cell_type":"code","source":"X_scaled = scaler.transform(X)\ny_preds = lr.predict(X_scaled)\n\nyellow_taxi_preds = df[['mean_trip_duration', 'mean_trip_distance']].copy()\nyellow_taxi_preds['predicted_fare'] = y_preds\n\ndf['predicted_fare'] = y_preds\ndf.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"drop_cols = ['Unnamed: 0', 'tpep_pickup_datetime', 'tpep_dropoff_datetime',\n             'payment_type', 'trip_distance', 'trip_duration', 'PULocationID', 'DOLocationID',\n             'extra', 'mta_tax', 'tip_amount', 'tolls_amount', 'store_and_fwd_flag',\n             'improvement_surcharge', 'total_amount', 'month_name', 'month_num', 'day_num', 'predicted_fare']\n\ndf1 = df.drop(drop_cols, axis = 1)\n\ncols_to_str = ['RatecodeID', 'VendorID']\n\nfor col in cols_to_str:\n    df[col] = df[col].astype('str')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df1 = pd.get_dummies(df, drop_first = False)\ndf1.info()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"features = df1.drop('fare_amount', axis = 1)\ntarget = df1['fare_amount']\n\n# Split the dataset into training and testing sets with stratification\nX_train, X_test, y_train, y_test = train_test_split(\n    features,\n    target,\n    test_size=0.20,\n    stratify=target,\n    random_state=42\n)\n\n# Standardize features using Z-score normalization\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\n# ##  6. Model Definitions and Hyperparameter Grids\n\n# Define cross-validation strategy\ncv = KFold(n_splits=5, shuffle=True, random_state=42)\n\n# Initialize models with relevant configurations\nmodels = {\n    'Decision Tree': DecisionTreeRegressor(random_state=42),\n    'Random Forest': RandomForestRegressor(random_state=42),\n    'XGBoost': xgb.XGBRegressor(\n        objective='reg:squarederror',\n        use_label_encoder=False,\n        random_state=42\n    )\n}\n\n# Define hyperparameter grids for each model\nparam_grids = {\n    'Decision Tree': {\n        'max_depth': [1, 4, None],\n        'min_samples_leaf': [2, 5, 10],\n        'min_samples_split': [2, 10]\n    },\n    'Random Forest': {\n        'max_depth': [1, 4, None],\n        'min_samples_leaf': [2, 5, 10],\n        'min_samples_split': [2, 10],\n        'max_features': [0.5, 1.0],\n        'max_samples': [0.7, 1.0],\n        'n_estimators': [50, 200]\n    },\n    'XGBoost': {\n        'max_depth': [1, 4, 6],\n        'subsample': [0.5, 1.0],\n        'min_child_weight': [2, 5],\n        'learning_rate': [0.1, 0.2],\n        'n_estimators': [50, 200]\n    }\n}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\n# ##  7. Training and Hyperparameter Optimization\n\nbest_models = {}\n\nfor name, model in models.items():\n    if name in param_grids:\n        # Perform grid search with cross-validation\n        grid = GridSearchCV(\n            estimator=model,\n            param_grid=param_grids[name],\n            scoring='r2',\n            cv=cv,\n            n_jobs=-1,\n            verbose=0\n        )\n        grid.fit(X_train_scaled, y_train)\n        best_models[name] = grid.best_estimator_\n        print(f\"{name} best parameters: {grid.best_params_}\\n\")\n    else:\n        # Train model directly without tuning\n        model.fit(X_train_scaled, y_train)\n        best_models[name] = model","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}